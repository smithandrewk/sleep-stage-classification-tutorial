{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 ['A1-0', 'A1-1', 'A4-0', 'B1-0', 'B3-1', 'C1-0', 'C4-0', 'C4-1', 'D1-0', 'E1-0', 'E2-1', 'E4-0', 'E4-1', 'F1-0', 'F1-1', 'F5-1']\n",
      "58 ['21-HET-1', '21-HET-10', '21-HET-11', '21-HET-12', '21-HET-13', '21-HET-2', '21-HET-3', '21-HET-4', '21-HET-5', '21-HET-7', '21-HET-8', '21-HET-9', '21-KO-1', '21-KO-10', '21-KO-11', '21-KO-12', '21-KO-2', '21-KO-3', '21-KO-4', '21-KO-5', '21-KO-6', '21-KO-7', '21-KO-8', '21-KO-9', '21-WK-1', '21-WK-10', '21-WK-11', '21-WK-12', '21-WK-13', '21-WK-15', '21-WK-16', '21-WK-17', '21-WK-18', '21-WK-2', '21-WK-3', '21-WK-4', '21-WK-5', '21-WK-6', '21-WK-8', '21-WK-9', '21-WT-1', '21-WT-10', '21-WT-12', '21-WT-13', '21-WT-2', '21-WT-3', '21-WT-4', '21-WT-5', '21-WT-6', '21-WT-7', '21-WT-8', '21-WT-9', '354', '378', '381', '382', '386', '429']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import TensorDataset,ConcatDataset,DataLoader,WeightedRandomSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.nn.functional import relu\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import ConfusionMatrixDisplay,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def moving_average(data, window_size=10):\n",
    "    return np.convolve(data, np.ones(window_size), 'valid') / window_size\n",
    "\n",
    "colors = {\n",
    "    'Train': '#007AFF',  # Apple Blue\n",
    "    'Test': '#FF9500'    # Apple Orange\n",
    "}\n",
    "\n",
    "device = 'cuda'\n",
    "path_to_pt_ekyn = f'../../pt_ekyn'\n",
    "path_to_pt_snezana_mice = f'../../pt_snezana_mice'\n",
    "\n",
    "ekyn_ids = sorted(set([recording_filename.split('_')[0] for recording_filename in os.listdir(path_to_pt_ekyn)]))\n",
    "snezana_mice_ids = sorted(set([recording_filename.split('.')[0] for recording_filename in os.listdir(path_to_pt_snezana_mice)]))\n",
    "print(len(ekyn_ids),ekyn_ids)\n",
    "print(len(snezana_mice_ids),snezana_mice_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = torch.load(f'{path_to_pt_ekyn}/{id}_PF.pt',weights_only=False)\n",
    "X.shape,y.shape\n",
    "trainloader = DataLoader(TensorDataset(X,y),batch_size=32,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = torch.load(f'{path_to_pt_snezana_mice}/{snezana_mice_ids[0]}.pt',weights_only=False)\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with h5py.File(\"eeg_data.h5\", \"w\") as f:\n",
    "#     ekyn_group = f.create_group(\"ekyn\")\n",
    "#     for i,id in enumerate(ekyn_ids):\n",
    "#         rat_group = ekyn_group.create_group(f\"{i}\")\n",
    "#         for j,condition in enumerate(['PF','Vehicle']):\n",
    "#             condition_group = rat_group.create_group(f\"{j}\")\n",
    "#             X, y = torch.load(f'{path_to_pt_ekyn}/{id}_{condition}.pt',weights_only=False)\n",
    "#             X = X.unsqueeze(1)\n",
    "#             X = X[:,:,::10] # 500 Hz -> 50 Hz\n",
    "#             condition_group.create_dataset(\"X\", data=X)\n",
    "#             condition_group.create_dataset(\"y\", data=y)\n",
    "\n",
    "#             condition_group.attrs[\"rat_id\"] = i\n",
    "#             condition_group.attrs[\"raw_rat_id\"] = id\n",
    "#             condition_group.attrs[\"condition_id\"] = j\n",
    "#             condition_group.attrs[\"raw_condition_id\"] = condition\n",
    "\n",
    "#     snezana_mice_group = f.create_group(\"mice\")\n",
    "#     for i,id in enumerate(snezana_mice_ids):\n",
    "#         mice_group = snezana_mice_group.create_group(f\"{i}\")\n",
    "#         X, y = torch.load(f'{path_to_pt_snezana_mice}/{id}.pt',weights_only=False)\n",
    "#         X = X.unsqueeze(1)\n",
    "#         X = X[:,:,::10] # 500 Hz -> 50 Hz\n",
    "#         mice_group.create_dataset(\"X\", data=X)\n",
    "#         mice_group.create_dataset(\"y\", data=y)\n",
    "\n",
    "#         mice_group.attrs[\"mouse_id\"] = i\n",
    "#         mice_group.attrs[\"raw_mouse_id\"] = id\n",
    "\n",
    "\n",
    "# print(\"Data saved to eeg_data.h5!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(\"eeg_data.h5\", \"r\") as f:\n",
    "    trainloader = DataLoader(\n",
    "        ConcatDataset(\n",
    "            [\n",
    "                TensorDataset(torch.from_numpy(f['ekyn'][key]['0']['X'][:]),torch.from_numpy(f['ekyn'][key]['0']['y'][:])) for key in list(f['ekyn'].keys())[:8]\n",
    "            ]\n",
    "        )\n",
    "        ,batch_size=512,shuffle=True)\n",
    "    testloader = DataLoader(\n",
    "        ConcatDataset(\n",
    "            [\n",
    "                TensorDataset(torch.from_numpy(f['mice'][key]['X'][:]),torch.from_numpy(f['mice'][key]['y'][:])) for key in list(f['mice'].keys())[:2]\n",
    "            ]\n",
    "        )\n",
    "        ,batch_size=512,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 1, 500]) torch.Size([512, 3]) tensor([ 33, 236, 243])\n",
      "torch.Size([512, 1, 500]) torch.Size([512, 3]) tensor([ 16, 187, 309])\n"
     ]
    }
   ],
   "source": [
    "Xi,yi = next(iter(trainloader))\n",
    "print(Xi.shape,yi.shape,yi.argmax(dim=1).bincount())\n",
    "\n",
    "Xi,yi = next(iter(testloader))\n",
    "print(Xi.shape,yi.shape,yi.argmax(dim=1).bincount())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNSleepStager(\n",
       "  (norm): SimpleNorm()\n",
       "  (c1): Conv1d(1, 4, kernel_size=(7,), stride=(1,), padding=same)\n",
       "  (c2): Conv1d(4, 8, kernel_size=(5,), stride=(1,), padding=same)\n",
       "  (c3): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=same)\n",
       "  (c4): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=same)\n",
       "  (mp): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "  (fc1): Linear(in_features=32, out_features=16, bias=True)\n",
       "  (classifier): Linear(in_features=16, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleNorm(nn.Module):\n",
    "    def __init__(self,eps):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.tensor(1.0))\n",
    "        self.shift = nn.Parameter(torch.tensor(0.0))\n",
    "    def forward(self,x):\n",
    "        mean = x.flatten().mean()\n",
    "        std = x.flatten().std()\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "        return x * self.scale + self.shift\n",
    "    \n",
    "class CNNSleepStager(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.norm = SimpleNorm(1e-5)\n",
    "        self.c1 = nn.Conv1d(in_channels=1,out_channels=4,kernel_size=7,padding='same')\n",
    "        self.c2 = nn.Conv1d(in_channels=4,out_channels=8,kernel_size=5,padding='same')\n",
    "        self.c3 = nn.Conv1d(in_channels=8,out_channels=16,kernel_size=3,padding='same')\n",
    "        self.c4 = nn.Conv1d(in_channels=16,out_channels=32,kernel_size=3,padding='same')\n",
    "\n",
    "        self.mp = nn.MaxPool1d(kernel_size=2)\n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=32,out_features=16)\n",
    "        self.classifier = nn.Linear(in_features=16,out_features=3)\n",
    "    def forward(self,x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        x = self.c1(x)\n",
    "        x = relu(x)\n",
    "        x = self.mp(x)\n",
    "\n",
    "        x = self.c2(x)\n",
    "        x = relu(x)\n",
    "        x = self.mp(x)\n",
    "\n",
    "        x = self.c3(x)\n",
    "        x = relu(x)\n",
    "        x = self.mp(x)\n",
    "\n",
    "        x = self.c4(x)\n",
    "        x = relu(x)\n",
    "        x = self.mp(x)\n",
    "\n",
    "        x = self.gap(x)\n",
    "        x = x.squeeze()\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "model = CNNSleepStager()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=3e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|‚ñè         | 64/5000 [00:38<49:53,  1.65it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdark_background\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5000\u001b[39m)):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m Xi,yi \u001b[38;5;129;01min\u001b[39;00m trainloader:\n\u001b[1;32m     13\u001b[0m         Xi,yi \u001b[38;5;241m=\u001b[39m Xi\u001b[38;5;241m.\u001b[39mto(device),yi\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataset.py:338\u001b[0m, in \u001b[0;36mConcatDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_sizes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m-\u001b[39midx \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainlossi = []\n",
    "testlossi = []\n",
    "\n",
    "window_size = 10\n",
    "validation_frequency_epochs = 20\n",
    "best_dev_loss = torch.inf\n",
    "best_dev_loss_epoch = 0\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "for epoch in tqdm(range(5000)):\n",
    "    for Xi,yi in trainloader:\n",
    "        Xi,yi = Xi.to(device),yi.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(Xi)\n",
    "        loss = criterion(logits,yi)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        trainlossi.append(loss.item())\n",
    "\n",
    "    if epoch % validation_frequency_epochs == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            testlossi.append(torch.hstack([criterion(model(Xi.to(device)),yi.to(device)).cpu() for Xi,yi in testloader]).mean().item())\n",
    "\n",
    "        if testlossi[-1] < best_dev_loss:\n",
    "            best_dev_loss = testlossi[-1]\n",
    "            best_dev_loss_epoch = epoch\n",
    "\n",
    "        fig,axes = plt.subplots(nrows=1,ncols=1,figsize=(8,15),dpi=300)\n",
    "\n",
    "        # # Adjust figure background\n",
    "        # fig.patch.set_facecolor('black')\n",
    "\n",
    "        # # Adjust axes background\n",
    "        # axes.set_facecolor('black')\n",
    "\n",
    "        x_trainlossi = torch.linspace(0,(len(testlossi)-1)*validation_frequency_epochs,len(trainlossi))\n",
    "        x_testlossi = torch.linspace(0,(len(testlossi)-1)*validation_frequency_epochs,len(testlossi))\n",
    "\n",
    "        plt.plot(x_trainlossi,trainlossi, label='Train Loss', color=colors['Train'], alpha=0.4, linewidth=1.5)\n",
    "\n",
    "        if len(trainlossi) > window_size:\n",
    "            x_trainlossi_ma = torch.linspace(window_size-1,(len(testlossi)-1)*validation_frequency_epochs,len(trainlossi)-(window_size-1))\n",
    "            trainlossi_ma = moving_average(trainlossi, window_size)\n",
    "            plt.plot(x_trainlossi_ma, trainlossi_ma, label='Train Loss MA', color=colors['Train'], linestyle='--', linewidth=1.5)\n",
    "\n",
    "            plt.axvline(x=x_trainlossi_ma[trainlossi_ma.argmin()],color=colors['Train'], alpha=0.4)\n",
    "            min_trainlossi_ma = torch.tensor(trainlossi_ma).min()\n",
    "            plt.axhline(y=min_trainlossi_ma,color=colors['Train'], alpha=0.4)\n",
    "\n",
    "            # Add text on the right-hand side at the orange line value\n",
    "            plt.text(plt.xlim()[1] + .1, min_trainlossi_ma, f'{min_trainlossi_ma:.2f}', \n",
    "                    verticalalignment='center', horizontalalignment='left', color=colors['Train'], fontweight='bold')\n",
    "\n",
    "\n",
    "        plt.plot(x_testlossi,testlossi, label='Test Loss', color=colors['Test'], alpha=1, linewidth=1.5)\n",
    "\n",
    "        plt.axvline(x=x_testlossi[torch.tensor(testlossi).argmin()],color=colors['Test'], alpha=0.4)\n",
    "        min_testlossi = torch.tensor(testlossi).min()\n",
    "        plt.axhline(y=min_testlossi,color=colors['Test'], alpha=0.4)\n",
    "\n",
    "        # Add text on the right-hand side at the orange line value\n",
    "        plt.text(plt.xlim()[1] + .1, min_testlossi, f'{min_testlossi:.2f}', \n",
    "                verticalalignment='center', horizontalalignment='left', color=colors['Test'], fontweight='bold')\n",
    "        \n",
    "\n",
    "        plt.xlabel('epoch',fontweight='bold')\n",
    "        plt.ylabel('loss',fontweight='bold')\n",
    "\n",
    "        plt.ylim([0,1])\n",
    "        plt.savefig('loss.jpg',bbox_inches='tight')\n",
    "        plt.close()\n",
    "        model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.vstack([torch.vstack([model(Xi.to(device)).softmax(dim=1).argmax(dim=1).cpu(),yi.argmax(dim=1)]).T for Xi,yi in trainloader])\n",
    "y_pred = y[:,0]\n",
    "y_true = y[:,1]\n",
    "print(classification_report(y_true=y_true,y_pred=y_pred))\n",
    "\n",
    "y = torch.vstack([torch.vstack([model(Xi.to(device)).softmax(dim=1).argmax(dim=1).cpu(),yi.argmax(dim=1)]).T for Xi,yi in testloader])\n",
    "y_pred = y[:,0]\n",
    "y_true = y[:,1]\n",
    "print(classification_report(y_true=y_true,y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = sorted(set([recording_filename.split('_')[0] for recording_filename in os.listdir(path_to_pt_ekyn)]))\n",
    "\n",
    "X,y = torch.load(f'{path_to_pt_ekyn}/{ids[9]}_PF.pt',weights_only=False)\n",
    "X = X.unsqueeze(1)\n",
    "X = X[:,:,::10] # 500 Hz -> 50 Hz\n",
    "\n",
    "testloader = DataLoader(TensorDataset(X,y),batch_size=512,shuffle=True)\n",
    "\n",
    "Xi,yi = next(iter(testloader))\n",
    "print(Xi.shape,yi.shape,yi.argmax(dim=1).bincount())\n",
    "\n",
    "y = torch.vstack([torch.vstack([model(Xi.to(device)).softmax(dim=1).argmax(dim=1).cpu(),yi.argmax(dim=1)]).T for Xi,yi in testloader])\n",
    "y_pred = y[:,0]\n",
    "y_true = y[:,1]\n",
    "print(classification_report(y_true=y_true,y_pred=y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
